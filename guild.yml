- model: nmt
  description: NMT
  sourcecode:
    - '*.py'
    - '*.sh'
    - '*.pl'
    - '*.yml'
  operations:
    train:
      description: Train the RNN with Attention
      main: nmt
      flags:
        train_dataset:
          description: Path to training corpus in csv file
          default: "/home/chaitanya/Research/datasets/iwslt14.tokenized.de-en/iwslt14.tokenized.de-en/train.en-de.csv"
        valid_dataset:
          description: Path to validation corpus in csv file
          default: "/home/chaitanya/Research/datasets/iwslt14.tokenized.de-en/iwslt14.tokenized.de-en/valid.en-de.csv"
        test_dataset:
          description: Path to test corpus in csv file
          default: "/home/chaitanya/Research/datasets/iwslt14.tokenized.de-en/iwslt14.tokenized.de-en/test.en-de.csv"
        adagram_embeddings_dir:
          description: Directory containing adagram vectors
          default: "/home/chaitanya/Research/ShataAnuvadak/_pytorch/all_indian_languages/data/monolingual/english/wiki9/"
        testset_target:
          description: Directory containing adagram vectors
          default: "/home/chaitanya/Research/datasets/iwslt14.tokenized.de-en/iwslt14.tokenized.de-en/test.de"
        save_to:
          description: Path to test corpus in csv file
          default: "data/model.pt"
        path_to_logs:
          default: "data/training_logs.txt"
        save_freq:
          default: 10000000
        source_max_len:
          default: 50
        target_max_len:
          default: 50
        source_max_vocab_size:
          default: 46000
        target_max_vocab_size:
          default: 60000
#        use_epochs:
#          arg-switch: no
        ignore_too_many_unknowns:
          default: 1
        bidirectional:
          default: 1
        encoder_num_layers:
          default: 2
        encoder_hidden_dim:
          default: 500
        encoder_embedding_dim:
          default: 500
        encoder_dropout:
          default: 0.4
        decoder_num_layers:
          default: 2
        decoder_hidden_dim:
          default: 500
        decoder_embedding_dim:
          default: 500
        decoder_dropout:
          default: 0.4
        decoder_attention_type:
          default: "general"
        optimizer:
          default: "sgd"
        lrate:
          default: 1.0
        max_grad_norm:
          default: 5.0
        start_decay_at:
          default: 15000
        decay_lrate_steps:
          default: 1000
        steps:
          default: 30000
        lrate_decay:
          default: 0.5
#        TODO: check warmup steps
        warmup_steps:
          default: 0

#        TODO: Fix the adam_beta1 and adam_beta2 values
        adam_beta1:
          default: 0.9
        adam_beta2:
          default: 0.999

        batch_size:
          default: 64

        norm_method:
          default: "sents"
        model_type:
#         Allowed options:
#          - seq2seq_baseline
#          - seq2seq_baseline_word_attn
#          - seq2seq_multivec
#          - seq2seq_multivec_word_attn
          default: "seq2seq_multivec_word_attn"
        valid_steps:
          default: 250
        patience_steps:
          default: 6

        expt_name:
          default: "seq2seq_multivec_word_attn"

  references:
    - Some ref
